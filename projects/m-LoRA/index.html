<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>m-LoRA | IDs Lab | SCU</title> <meta name="author" content="IDs Lab Artificial Intelligence + Database + System"> <meta name="description" content="Efficient LLM Model Fine-Tune via Multi-LoRA Optimization"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ids_lab_logo_icon_black.svg?cafe3f11dba4c6936396520357d301f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ids-lab-scu.github.io/projects/m-LoRA/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">IDs Lab | SCU</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">m-LoRA</h1> <p class="post-description">Efficient LLM Model Fine-Tune via Multi-LoRA Optimization</p> </header> <article> <h2 id="m-lora-efficient-llm-model-fine-tune-via-multi-lora-optimization">m-LoRA: Efficient LLM Model Fine-Tune via Multi-LoRA Optimization</h2> <p><a href="https://github.com/TUDB-Labs/multi-lora-fine-tune/actions/workflows/python-test-main.yml" rel="external nofollow noopener" target="_blank"><img src="https://github.com/TUDB-Labs/multi-lora-fine-tune/actions/workflows/python-test-main.yml/badge.svg" alt=""></a> <a href="https://github.com/TUDB-Labs/multi-lora-fine-tune/stargazers" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/github/stars/TUDB-Labs/multi-lora-fine-tune?logo=GitHub" alt=""></a> <a href="http://www.apache.org/licenses/LICENSE-2.0" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/github/license/TUDB-Labs/multi-lora-fine-tune" alt=""></a> <a href="https://github.com/TUDB-Labs/multi-lora-fine-tune/releases/latest" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/github/v/release/TUDB-Labs/multi-lora-fine-tune" alt=""></a> <a href="https://www.python.org/" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/github/languages/top/TUDB-Labs/multi-lora-fine-tune" alt=""></a></p> <p>m-LoRA (a.k.a Multi-Lora Fine-Tune) is an open-source framework for fine-tuning Large Language Models (LLMs) using the efficient multiple LoRA/QLoRA methods. Key features of m-LoRA include:</p> <ul> <li> <p>Efficient LoRA/QLoRA: Optimizes the fine-tuning process, significantly reducing GPU memory usage by leveraging a shared frozen-based model.</p> </li> <li> <p>Multiple LoRA Adapters: Support for concurrent fine-tuning of multiple LoRA/QLoRA adapters.</p> <h2 id="overview">Overview</h2> </li> </ul> <p><strong>m-LoRA</strong> is a high-throughput LLM fine-tuning framework based on LoRA and QLoRA, compatible with HuggingFace-Transformers LLaMA Models and ChatGLM Models.</p> <p>This picture shows the basic principle of LoRA and Multi-LoRA.</p> <div align="center">&lt;img src="./assets/img/projects/mlora/m-LoRA.png" width=70%"&gt;</div> <p>The system overview of m-LoRA is as follows.</p> <div align="center"><img src="./assets/img/projects/mlora/system_overview.png" width="100%"></div> <p>m-LoRA requires <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a> and <a href="https://developer.nvidia.com/cuda-toolkit" rel="external nofollow noopener" target="_blank">NVIDIA CUDA</a> compatible GPUs.</p> <h3 id="main-contribution">Main Contribution</h3> <ul> <li>Introduces the Multi-LoRA method, capable of enabling the sharing of pre-trained model weights during the fine-tuning process of large language models;</li> <li>Proposes a task scheduling algorithm to enhance the overall throughput of the task training process and reduce total training latency;</li> <li>Builds upon the above by implementing m-LoRA, a high-throughput large language model fine-tuning framework based on LoRA and QLoRA;</li> <li>Evaluates m-LoRA in experiments against existing systems, confirming that m-LoRA effectively utilizes system computing resources, thereby improving training throughput and reducing training latency compared to current systems.</li> </ul> <h3 id="experiment-results">Experiment Results</h3> <p>Environment: NVIDIA RTX A6000 with Intel Xeon Silver 4314 on Ubuntu 22.04.3</p> <p>Baseline: We utilized the widely adopted <a href="https://github.com/tloen/alpaca-lora" rel="external nofollow noopener" target="_blank">Alpaca-LoRA</a> as a foundation. On a single GPU, we independently ran multiple Alpaca-LoRA processes in parallel (marked as <em>Baseline@Alpaca-Parallel</em>) and sequentially (marked as <em>Baseline@Alpaca-Seq</em>), forming two baseline methods for the experiments. We test this on A100, and rest of results are based on the same GPU configure.</p> <h4 id="training-latency-and-throughput">Training Latency and Throughput</h4> <table> <thead> <tr> <th style="text-align: center">Method</th> <th style="text-align: center">Latency</th> <th style="text-align: center">Throughput</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Baseline@Alpaca-Seq</td> <td style="text-align: center">10.51h</td> <td style="text-align: center">608.41 token/s</td> </tr> <tr> <td style="text-align: center">Baseline@Alpaca-Parallel</td> <td style="text-align: center">9.85h</td> <td style="text-align: center">649.30 token/s</td> </tr> <tr> <td style="text-align: center">m-LoRA</td> <td style="text-align: center">9.46h</td> <td style="text-align: center">674.58 token/s</td> </tr> </tbody> </table> <p>We conducted four identical fine-tuning jobs with same dataset and same hyper-parameters, incorporating two baselines and m-LoRA. During the experimental process, we collected the completion times for each task in the baseline methods and calculated the time taken by the slowest task as the <em>Training Latency</em>. As shown in Table, m-LoRA exhibits lower <em>Training Latency</em> compared to both baseline methods. Specifically, m-LoRA is 9.99% faster than <em>Baseline@Alpaca-Seq</em> and 3.92% faster than <em>Baseline@Alpaca-Parallel</em>.</p> <div align="center"><img src="./assets/img/projects/mlora/throughput_compare.png" width="100%"></div> <h4 id="video-memory-usage">Video Memory Usage</h4> <div align="center"><img src="./assets/img/projects/mlora/GPU_memory_usage.png" width="100%"></div> <p>We conducted several fine-tuning jobs with same dataset and <code class="language-plaintext highlighter-rouge">batch_size = {2,4, 6, 8}</code>, incorporating <em>Baseline@Alpaca-Parallel</em> and m-LoRA.</p> <p><em>Baseline@Alpaca-Parallel</em> triggered OOM error after 3 parallel tasks when batch size = 8, while m-LoRA can handle twice that amount.</p> <h4 id="batching-strategies">Batching Strategies</h4> <table> <thead> <tr> <th style="text-align: center">Method</th> <th style="text-align: center">Training Latency</th> <th style="text-align: center">Peak Memory Usage</th> <th style="text-align: center">Average GPU Utilization</th> <th style="text-align: center">Training Throughput</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Baseline@Alpaca-Seq</td> <td style="text-align: center">27.73h</td> <td style="text-align: center">10.68GB</td> <td style="text-align: center">79.39%</td> <td style="text-align: center">653.35 token/s</td> </tr> <tr> <td style="text-align: center">m-LoRA@M1</td> <td style="text-align: center">36.82h</td> <td style="text-align: center">23.82GB</td> <td style="text-align: center">96.52%</td> <td style="text-align: center">672.54 token/s</td> </tr> <tr> <td style="text-align: center">m-LoRA@M2</td> <td style="text-align: center">39.14h</td> <td style="text-align: center">23.86GB</td> <td style="text-align: center">96.41%</td> <td style="text-align: center">671.28 token/s</td> </tr> <tr> <td style="text-align: center">m-LoRA@M3</td> <td style="text-align: center">22.97h</td> <td style="text-align: center">23.85GB</td> <td style="text-align: center">95.22%</td> <td style="text-align: center">674.41 token/s</td> </tr> </tbody> </table> <p>We conducted four fine-tuning jobs with different dataset but same hyper-parameters, incorporating <em>Baseline@Alpaca-Seq</em> and m-LoRA.</p> <p>During the experimental process, we collected following metrics:</p> <ul> <li> <em>Training Latency</em> = Job completion time</li> <li> <em>Throughput</em> = The number of passed tokens in model forward process / training latency</li> <li> <em>Memory Usage</em> = Peak video memory usage</li> <li> <em>GPU Utilization</em> = Average GPU utilization</li> </ul> <p>All metrics are computed for each job. <code class="language-plaintext highlighter-rouge">M1, M2, M3</code> represent three batch strategies of m-LoRA: <em>Optimal-Fit, Trivial, and Fast-Fit</em>. <code class="language-plaintext highlighter-rouge">BASELINE</code> denotes <em>Baseline@Alpaca-Seq</em>.</p> <p>The <em>Optimal-Fit</em> strategy performs the best across all four metrics, while the other two strategies also outperform the baseline method other than training latency.</p> <h3 id="use-cases">Use Cases:</h3> <ul> <li>Domain-Specific Fine-Tuning: This involves adapting a single model with various parameters particularly for one domain.</li> <li>Cross-Domain Fine-Tuning: This method leverages the base model to fine-tune multiple models, each intended for a different domain.</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 IDs Lab Artificial Intelligence + Database + System. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>